{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape_trustpilot_reviews\n",
    "import scrape_google_reviews\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Google Reviews scraping process...\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "# Step 1: Get establishments to scrape\n",
    "print(\"\\nStep 1: Getting establishments to scrape...\")\n",
    "establishments = scrape_google_reviews.get_establishments_to_scrape()\n",
    "\n",
    "if establishments.empty:\n",
    "    print(\"No establishments found matching the criteria.\")\n",
    "else:\n",
    "    print(f\"Found {len(establishments)} establishments to scrape:\")\n",
    "    display(establishments[['placeId', 'title', 'website', 'reviewsCount']])\n",
    "    \n",
    "    # Step 2: Scrape reviews\n",
    "    print(\"\\nStep 2: Scraping reviews...\")\n",
    "    reviews_df = scrape_google_reviews.scrape_reviews(establishments)\n",
    "    print(f\"Scraped {len(reviews_df)} reviews\")\n",
    "    if not reviews_df.empty:\n",
    "        print(\"\\nFirst few reviews:\")\n",
    "        display(reviews_df.head())\n",
    "    \n",
    "    # Step 3: Save individual review files\n",
    "    print(\"\\nStep 3: Saving individual review files...\")\n",
    "    scrape_google_reviews.save_reviews(reviews_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Unifying reviews...\n",
      "Found existing unified file: reviews\\google\\allGoogleReviews_2025-04-05.xlsx\n",
      "Previously processed files: 3\n",
      "Processing new file: reviews\\google\\googleReviews_2025-04-04_23-13.xlsx\n",
      "Processed 1 new files\n",
      "\n",
      "Saved unified reviews to: reviews/google/allGoogleReviews_2025-04-05.xlsx\n",
      "\n",
      "Step 5: Updating establishment base...\n",
      "\n",
      "Process completed successfully!\n",
      "End timestamp: 2025-04-05 09:17:13\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Unify reviews\n",
    "print(\"\\nStep 4: Unifying reviews...\")\n",
    "scrape_google_reviews.unify_reviews()\n",
    "\n",
    "# Step 5: Update establishment base\n",
    "print(\"\\nStep 5: Updating establishment base...\")\n",
    "scrape_google_reviews.update_establishment_base()\n",
    "\n",
    "print(\"\\nProcess completed successfully!\")\n",
    "print(f\"End timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reviews\\google\\googleReviews_2025-04-03_13-30.xlsx\n",
      "Updated reviews\\google\\googleReviews_2025-04-03_13-30.xlsx\n",
      "Processing reviews\\google\\googleReviews_2025-04-03_18-56.xlsx\n",
      "Updated reviews\\google\\googleReviews_2025-04-03_18-56.xlsx\n",
      "Processing reviews\\google\\googleReviews_2025-04-04_23-13.xlsx\n",
      "Updated reviews\\google\\googleReviews_2025-04-04_23-13.xlsx\n",
      "Processing reviews\\google\\googleReviews_2025-04-04_23-25.xlsx\n",
      "Updated reviews\\google\\googleReviews_2025-04-04_23-25.xlsx\n",
      "Processing reviews\\google\\allGoogleReviews_2025-04-05.xlsx\n",
      "Updated reviews\\google\\allGoogleReviews_2025-04-05.xlsx\n",
      "All files have been processed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def generate_review_id(place_id, published_at):\n",
    "    return hashlib.md5(f\"{place_id}_{published_at}\".encode()).hexdigest()\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing {file_path}\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Generate reviewId for each row\n",
    "    df['reviewId'] = df.apply(\n",
    "        lambda row: generate_review_id(row['placeId'], row['publishedAtDate']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Save the updated file\n",
    "    df.to_excel(file_path, index=False)\n",
    "    print(f\"Updated {file_path}\")\n",
    "\n",
    "# Process all Google review files\n",
    "google_dir = Path(\"reviews/google\")\n",
    "for file in google_dir.glob(\"googleReviews_*.xlsx\"):\n",
    "    if not file.name.startswith(\"allGoogleReviews_\"):  # Skip unified files\n",
    "        process_file(file)\n",
    "\n",
    "# Process the unified file if it exists\n",
    "unified_files = list(google_dir.glob(\"allGoogleReviews_*.xlsx\"))\n",
    "if unified_files:\n",
    "    latest_unified = max(unified_files)\n",
    "    process_file(latest_unified)\n",
    "\n",
    "print(\"All files have been processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading googleReviews_2025-04-03_13-30.xlsx\n",
      "Reading googleReviews_2025-04-03_18-56.xlsx\n",
      "Reading googleReviews_2025-04-04_23-13.xlsx\n",
      "Reading googleReviews_2025-04-04_23-25.xlsx\n",
      "\n",
      "Total number of reviews across all files: 8467\n",
      "Number of unique reviews: 6663\n",
      "Number of duplicate reviews: 1804\n",
      "\n",
      "Analyzing duplicate patterns:\n",
      "\n",
      "Reviews appearing in multiple files:\n",
      "occurrences\n",
      "2     740\n",
      "3     121\n",
      "4      52\n",
      "5      16\n",
      "6      19\n",
      "7       7\n",
      "8       7\n",
      "9       3\n",
      "10      3\n",
      "11      1\n",
      "12      1\n",
      "13      5\n",
      "14      3\n",
      "15      2\n",
      "16      1\n",
      "18      1\n",
      "20      1\n",
      "21      1\n",
      "23      1\n",
      "29      1\n",
      "30      1\n",
      "68      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example of a duplicate review:\n",
      "                              reviewId  \\\n",
      "6774  00025b35461fc3a8f5b86bf061189b8a   \n",
      "6775  00025b35461fc3a8f5b86bf061189b8a   \n",
      "\n",
      "                                                   text      publishedAtDate  \\\n",
      "6774  Hi , I m kium\\nI heard about hair transplant l...  2021-08-13 00:00:00   \n",
      "6775  My experience is owesome from the day i land i...  2021-08-13 00:00:00   \n",
      "\n",
      "                              source_file  \n",
      "6774  googleReviews_2025-04-04_23-25.xlsx  \n",
      "6775  googleReviews_2025-04-04_23-25.xlsx  \n",
      "\n",
      "Checking for reviews with same reviewId but different content:\n",
      "\n",
      "Review 00025b35461fc3a8f5b86bf061189b8a has different content across files:\n",
      "                              source_file  \\\n",
      "6774  googleReviews_2025-04-04_23-25.xlsx   \n",
      "6775  googleReviews_2025-04-04_23-25.xlsx   \n",
      "\n",
      "                                                   text  stars  likesCount  \\\n",
      "6774  Hi , I m kium\\nI heard about hair transplant l...      5           0   \n",
      "6775  My experience is owesome from the day i land i...      5           0   \n",
      "\n",
      "                                  responseFromOwnerText  \n",
      "6774  Hello sır. Thnak you for your nıce comment. We...  \n",
      "6775                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Function to analyze duplicates\n",
    "def analyze_duplicates():\n",
    "    google_dir = Path(\"reviews/google\")\n",
    "    all_reviews = []\n",
    "    \n",
    "    # Read all individual review files\n",
    "    for file in google_dir.glob(\"googleReviews_*.xlsx\"):\n",
    "        if not file.name.startswith(\"allGoogleReviews_\"):  # Skip unified files\n",
    "            print(f\"Reading {file.name}\")\n",
    "            df = pd.read_excel(file)\n",
    "            df['source_file'] = file.name\n",
    "            all_reviews.append(df)\n",
    "    \n",
    "    if not all_reviews:\n",
    "        print(\"No review files found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine all reviews\n",
    "    combined_df = pd.concat(all_reviews, ignore_index=True)\n",
    "    \n",
    "    # Count total reviews\n",
    "    total_reviews = len(combined_df)\n",
    "    print(f\"\\nTotal number of reviews across all files: {total_reviews}\")\n",
    "    \n",
    "    # Count unique reviews\n",
    "    unique_reviews = combined_df['reviewId'].nunique()\n",
    "    print(f\"Number of unique reviews: {unique_reviews}\")\n",
    "    \n",
    "    # Calculate duplicates\n",
    "    duplicates = total_reviews - unique_reviews\n",
    "    print(f\"Number of duplicate reviews: {duplicates}\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(\"\\nAnalyzing duplicate patterns:\")\n",
    "        # Find reviews that appear in multiple files\n",
    "        duplicate_reviews = combined_df[combined_df.duplicated(subset=['reviewId'], keep=False)]\n",
    "        \n",
    "        # Group by reviewId and count occurrences\n",
    "        duplicate_counts = duplicate_reviews.groupby('reviewId').size().reset_index(name='occurrences')\n",
    "        duplicate_counts = duplicate_counts[duplicate_counts['occurrences'] > 1]\n",
    "        \n",
    "        print(f\"\\nReviews appearing in multiple files:\")\n",
    "        print(duplicate_counts['occurrences'].value_counts().sort_index())\n",
    "        \n",
    "        # Show example of a duplicate review\n",
    "        print(\"\\nExample of a duplicate review:\")\n",
    "        example_review_id = duplicate_counts.iloc[0]['reviewId']\n",
    "        example_review = combined_df[combined_df['reviewId'] == example_review_id]\n",
    "        print(example_review[['reviewId', 'text', 'publishedAtDate', 'source_file']])\n",
    "    \n",
    "    # Check for reviews with same reviewId but different content\n",
    "    print(\"\\nChecking for reviews with same reviewId but different content:\")\n",
    "    review_groups = combined_df.groupby('reviewId')\n",
    "    for review_id, group in review_groups:\n",
    "        if len(group) > 1:\n",
    "            # Check if any of these columns differ\n",
    "            diff_columns = ['text', 'stars', 'likesCount', 'responseFromOwnerText']\n",
    "            if group[diff_columns].nunique().any():\n",
    "                print(f\"\\nReview {review_id} has different content across files:\")\n",
    "                print(group[['source_file'] + diff_columns])\n",
    "                break  # Just show the first example\n",
    "\n",
    "# Run the analysis\n",
    "analyze_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing unified file: allGoogleReviews_2025-04-05.xlsx\n",
      "Number of reviews in unified file: 2922\n",
      "\n",
      "Reading googleReviews_2025-04-03_13-30.xlsx\n",
      "Number of reviews in googleReviews_2025-04-03_13-30.xlsx: 496\n",
      "\n",
      "Reading googleReviews_2025-04-03_18-56.xlsx\n",
      "Number of reviews in googleReviews_2025-04-03_18-56.xlsx: 344\n",
      "\n",
      "Reading googleReviews_2025-04-04_23-13.xlsx\n",
      "Number of reviews in googleReviews_2025-04-04_23-13.xlsx: 3742\n",
      "\n",
      "Reading googleReviews_2025-04-04_23-25.xlsx\n",
      "Number of reviews in googleReviews_2025-04-04_23-25.xlsx: 3885\n",
      "\n",
      "Total number of reviews across all individual files: 6663\n",
      "\n",
      "Number of reviews missing from unified file: 3741\n",
      "\n",
      "Example of missing reviews:\n",
      "                             reviewId  \\\n",
      "840  3600819d589df7232a217a151d6ff9a6   \n",
      "841  3a31a8e89c49295f47dc40e802b9c9ee   \n",
      "842  188f31136d94b4e604330789cfbf6d66   \n",
      "843  35e1ee2e662dfb3210620e592191b1d8   \n",
      "844  0eab2b332674d42b6c8a903ed1010bcd   \n",
      "\n",
      "                                                  text  \\\n",
      "840  I had my hair transplant at Hermest Clinic in ...   \n",
      "841  Couldn’t recommend the experience more. I was ...   \n",
      "842  The team was very helpful and supported me in ...   \n",
      "843  Ich wollte seit Jahren unbedingt Haar transpla...   \n",
      "844  Very good experience during my visit and treat...   \n",
      "\n",
      "              publishedAtDate                          source_file  \n",
      "840  2025-04-04T09:22:52.216Z  googleReviews_2025-04-04_23-13.xlsx  \n",
      "841  2025-04-03T23:07:49.520Z  googleReviews_2025-04-04_23-13.xlsx  \n",
      "842  2025-04-03T20:43:08.855Z  googleReviews_2025-04-04_23-13.xlsx  \n",
      "843  2025-04-03T20:35:10.137Z  googleReviews_2025-04-04_23-13.xlsx  \n",
      "844  2025-04-03T19:26:42.231Z  googleReviews_2025-04-04_23-13.xlsx  \n",
      "\n",
      "Files containing missing reviews:\n",
      "source_file\n",
      "googleReviews_2025-04-04_23-13.xlsx    3741\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def analyze_unified_file():\n",
    "    google_dir = Path(\"reviews/google\")\n",
    "    \n",
    "    # Find the latest unified file\n",
    "    unified_files = list(google_dir.glob(\"allGoogleReviews_*.xlsx\"))\n",
    "    if not unified_files:\n",
    "        print(\"No unified file found!\")\n",
    "        return\n",
    "    \n",
    "    latest_unified = max(unified_files)\n",
    "    print(f\"Analyzing unified file: {latest_unified.name}\")\n",
    "    \n",
    "    # Read the unified file\n",
    "    unified_df = pd.read_excel(latest_unified)\n",
    "    unified_review_ids = set(unified_df['reviewId'])\n",
    "    print(f\"Number of reviews in unified file: {len(unified_review_ids)}\")\n",
    "    \n",
    "    # Read all individual files\n",
    "    all_reviews = []\n",
    "    for file in google_dir.glob(\"googleReviews_*.xlsx\"):\n",
    "        if not file.name.startswith(\"allGoogleReviews_\"):  # Skip unified files\n",
    "            print(f\"\\nReading {file.name}\")\n",
    "            df = pd.read_excel(file)\n",
    "            df['source_file'] = file.name\n",
    "            all_reviews.append(df)\n",
    "            print(f\"Number of reviews in {file.name}: {len(df)}\")\n",
    "    \n",
    "    if not all_reviews:\n",
    "        print(\"No individual review files found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine all individual reviews\n",
    "    combined_df = pd.concat(all_reviews, ignore_index=True)\n",
    "    all_review_ids = set(combined_df['reviewId'])\n",
    "    print(f\"\\nTotal number of reviews across all individual files: {len(all_review_ids)}\")\n",
    "    \n",
    "    # Find missing reviews\n",
    "    missing_reviews = all_review_ids - unified_review_ids\n",
    "    print(f\"\\nNumber of reviews missing from unified file: {len(missing_reviews)}\")\n",
    "    \n",
    "    if missing_reviews:\n",
    "        print(\"\\nExample of missing reviews:\")\n",
    "        missing_df = combined_df[combined_df['reviewId'].isin(missing_reviews)]\n",
    "        print(missing_df[['reviewId', 'text', 'publishedAtDate', 'source_file']].head())\n",
    "        \n",
    "        # Analyze which files contain the missing reviews\n",
    "        print(\"\\nFiles containing missing reviews:\")\n",
    "        missing_files = missing_df['source_file'].value_counts()\n",
    "        print(missing_files)\n",
    "\n",
    "# Run the analysis\n",
    "analyze_unified_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
